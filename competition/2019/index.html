<!DOCTYPE html>
<html>
<head>
	<meta charset="UTF-8">
	<title>QComp 2019 - qcomp.org</title>
	<link href="https://fonts.googleapis.com/css?family=Tajawal:400,700" rel="stylesheet">
	<link rel="stylesheet" type="text/css" href="../../style.css">
	<script type="text/javascript" src="../../script.js"></script>
</head>
<body>

<h1>QComp 2019</h1>
<div class="belowh1">
	<i>Jump to:</i>&nbsp;&nbsp;<a href="#timeline">Timeline</a> &nbsp;|&nbsp; <a href="#participation">Participation</a> &nbsp;|&nbsp; <a href="#evaluation">Evaluation Procedure</a> &nbsp;|&nbsp; <a href="#organisers">Organisers</a>
</div>
<p>
	The <b>2019 Comparison of Tools for the Analysis of Quantitative Formal Models</b> (<a href="../../index.html">QComp</a> 2019) is part of the <a href="https://tacas.info/toolympics.php">TACAS 2019 Toolympics</a>.
	It is the first <b>friendly competition</b> among verification and analysis tools for quantitative formal models.
	Based on a curated subset of benchmarks from the <a href="../../benchmarks/index.html">Quantitative Formal Model Repository</a>, QComp will compare the performance, versatility and usability of the participating tools.
</p>


<h2 id="timeline">Timeline</h2>
<p>
	We expect QComp 2019 to roughly adhere to the following timeline.
	Exact dates will be fixed once the exact overall deadlines of the <a href="https://tacas.info/toolympics.php">TACAS Toolympics</a> have been announced.
</p>
<ul>
	<li>
		Nov 2018:
		Organisers and participants select benchmarks from the <a href="../../benchmarks/index.html">Quantitative Formal Model Repository</a>.
	</li>
	<li>
		Dec 2018:
		Participants deliver tool packages and instructions/benchmarking scripts for testing.
		Organisers provide feedback (errors, or performance data and results).
	</li>
	<li>
		<b>Jan 6, 2019</b>:
		Participants deliver the final tool package and instructions/benchmarking scripts for the official comparison.
	</li>
	<li>
		Jan 2019:
		Organisers run the performance comparison.
	</li>
	<li>
		Jan 2019:
		Participants provide a description of their tool w.r.t. its versatility and usability.
	</li>
	<li>
		Feb 2019:
		Organisers and participants finalise the competition report.
	</li>
</ul>


<h2 id="participation">Participation</h2>
<p>
	We want to include as many tools as possible:
	Every tool that is able to solve a significant subset of the <a href="../../benchmarks/index.html">benchmarks</a> is welcome, even if it may not support all model types, the JANI format, or all included kinds of properties.
	For example, a tool specialising in the analysis of stochastic Petri nets will not be expected to solve JANI Markov chain models.
	Every participating tool must be associated to exactly one participant, who we expect to co-author the competition report.
	A participant may participate with multiple tools, or multiple variants (analysis engines) of the same tool.
</p>
<p>
	If you would like to participate in QComp 2019, please contact <script type="text/javascript">WriteMailLink('n.etnewtu@snnamtrah.a:otliaml', 'nnamtraH dnrAs');</script> to be added to the list of current participants:
</p>
<table class="compact">
	<thead>
		<tr>
			<th>Participant</th>
			<th>Tools</th>
		</tr>
	</thead>
	<tbody>
		<tr>
			<td>
				<a href="http://iscasmc.ios.ac.cn/?p=1241">Ernst Moritz Hahn</a>
			</td>
			<td>
				<a href="https://github.com/liyi-david/ePMC"><b>ePMC</b></a>
				[<a href="https://doi.org/10.1007/978-3-319-06410-9_22" class="reference">HLSTZ14</a>]:
				an extensible probabilistic model checker for CTMC and MDP specified in PRISM syntax or JANI, formerly IscasMC
			</td>
		</tr>
		<tr>
			<td rowspan="2">
				<a href="http://arnd.hartmanns.name/">Arnd Hartmanns</a>
			</td>
			<td>
				<a href="http://www.modestchecker.net/"><b>mcsta</b></a>
				 [<a href="https://doi.org/10.1007/978-3-319-24953-7_10" class="reference">HH15</a>]:
				a disk-based explicit-state model checker for MA and PTA specified in Modest or JANI, part of the <a href="http://www.modestchecker.net/">Modest Toolset</a> [<a href="https://doi.org/10.1007/978-3-642-54862-8_51" class="reference">HH14</a>]
			</td>
		</tr>
		<tr>
			<td>
				<a href="http://www.modestchecker.net/"><b>modes</b></a>
				[<a href="https://doi.org/10.1007/978-3-319-89963-3_20" class="reference">BDHS18</a>]:
				a statistical model checker for MA and PTA specified in Modest or JANI, part of the <a href="http://www.modestchecker.net/">Modest Toolset</a> [<a href="https://doi.org/10.1007/978-3-642-54862-8_51" class="reference">HH14</a>]
			</td>
		</tr>
		<tr>
			<td>
				<a href="https://depend.cs.uni-saarland.de/~klauck/">Michaela Klauck</a>
			</td>
			<td>
				<a href="https://depend.cs.uni-saarland.de/~klauck/"><b>FRET-π LRTDP</b></a>:
				a model checker based on probabilistic planning algorithms for MDP specified in Modest or JANI, using infrastructure of the <a href="http://www.modestchecker.net/">Modest Toolset</a> [<a href="https://doi.org/10.1007/978-3-642-54862-8_51" class="reference">HH14</a>]
			</td>
		</tr>
		<tr>
			<td>
				<a href="https://moves.rwth-aachen.de/people/hensel/">Christian Hensel</a>
			</td>
			<td>
				<a href="http://www.stormchecker.org/"><b>Storm</b></a>
				[<a href="https://doi.org/10.1007/978-3-319-63390-9_31" class="reference">DJKV17</a>]:
				a high-performance model checker for MA specified in various modelling languages
			</td>
		</tr>
		<tr>
			<td>
				<a href="http://fai.cs.uni-saarland.de/steinmetz/">Marcel Steinmetz</a>
			</td>
			<td>
				<a href="http://fai.cs.uni-saarland.de/steinmetz/"><b>Probabilistic Fast Downward</b></a>
				[<a href="https://doi.org/10.1613/jair.5153" class="reference">SHB16</a>]:
				a probabilistic extension of the classical heuristic planner Fast Downward for MDP specified in PPDDL
			</td>
		</tr>
	</tbody>
</table>


<h2 id="evaluation">Evaluation Procedure</h2>
<p>
	QComp 2019 is a flexible and friendly event.
	As a <i>comparison</i>, not a competition, there will be no global scoring or ranking of tools.
	Instead, we will collect data and publish comparison tables and diagrams about the <b>performance</b> of the participating tools on a curated selection of models from the <a href="../../benchmarks/index.html">Quantitative Formal Model Repository</a>.
	We will analyse and highlight every tool's strengths and tradeoffs.
	Equally prominently, based on input and feedback from the participants, we will describe and compare the tools in terms of <b>versatility</b> and <b>usability</b>.
	The entire performance evaluation and report-writing process will be performed in close collaboration with the participants to quickly include feedback and resolve any setup errors or misunderstandings early.
	We expect all participants to co-author the competition report.
</p>
	
<h3>Performance Evaluation</h3>
<p>
	All tools will be evaluated by the organisers on a central server running an up-to-date 64-bit version of Ubuntu Linux 18.04.
	We expect to use a standard desktop machine with an Intel Core i7-920 CPU and 12 GB of RAM (subject to change).
	The organisers will make the benchmarking scripts that will be run on the central server available to participants for testing.
	Participants provide a tool package, installation instructions, and command lines/scripts to run the selected benchmarks.
	Additionally, for every benchmark, participants briefly explain the tool settings used, why they were chosen for the particular benchmark, and what the capabilities of their tool are (e.g. which properties of the benchmark are supported and what kind of guarantee is attached to the results provided by the tool).
	We expect tools to try to provide results with a relative error of ± 10e-4.
	Statistical tools (e.g. simulators) should provide results outside of this error bound with a probability of at most 5 %.
	The competition report will not contain a performance ranking, but will provide tables, diagrams, and a discussion reporting on the runtimes achieved by the tools.
	We will not directly evaluate memory usage, but note that the amount of RAM available in the central server is not high.
	Thus memory usage will be evaluated indirectly by considering for which (large) models a tool fails due to an out-of-memory situation.
</p>

<h3>Correctness of Results</h3>
<p>
	For quantitative benchmarks, it is often not clear what the true, correct numeric result for a property is.
	This is due to most tools that scale to large models using inexact floating-point arithmetic, and because any result may be affected by bugs in whatever "reference" tool is used.
	At the same time, it does not make sense to report performance results when a tool provides an incorrect result as this may be due to an error that drastically reduces or increases the analysis time.
	For this reason, QComp adopts the following pragmatic approach to evaluate the correctness of the results produced by the participating tools:
	During the benchmark selection phase, the organisers will use the most trustworthy analysis approach available (e.g. an exact-arithmetic solver for small and a model checker using a sound iterative numerical method for large models) to produce <i>reference results</i> for all selected models.
	Participants may use any other tool to try to refute the correctness of any of those reference results.
	If this happens, all participants will discuss and agree on a new reference result, or decide to exclude the affected benchmark.
	During the performance evaluation phase, every result that does not agree with the reference result up to the desired precision will be considered as incorrect.
	The number of incorrect results produced by a tool will be listed in the competition report.
</p>

<h3>Versatility and Usability</h3>
<p>
	As part of the competition report, we will also describe the versatility and usability of all participating tools, based on input from the participants.
	Versatility is, for example, the support for different input formats and formalisms, providing different and complementary analysis engines, and the tool's configurability (e.g. to make speed/precision tradeoffs).
	Usability is the quality of the tool's documentation (online, or provided as help text or messages within the tool), the availability of an easy-to-use graphical interface, how involved the installation process is, what platforms the tool runs on, whether it provides sensible default parameter settings, and so on.
</p>

<h3>Features of the Future</h3>
<p>
	As the first iteration of a quantitative analysis competition, QComp 2019 is intentionally limited to a small set of mathematical formalisms (from Markov chains to probabilistic timed automata) and properties (probabilistic reachability and expected rewards only).
	However, as part of the QComp 2019 evaluation, we also hope to identify advanced and upcoming areas of interest in quantitative modelling and analysis that may be of interest for the next edition of QComp.
</p>


<h2 id="organisers">Organisers</h2>
<ul class="none">
	<li>
		Organisation &amp; communication:
		<a href="http://arnd.hartmanns.name/">Arnd Hartmanns</a>
		<script type="text/javascript">WriteMailLinkHtml('n.etnewtu@snnamtrah.a:otliaml', '<img src="../../email.png" style="width: 13px; height: 11px; vertical-align: middle;" />');</script>
		(<a href="https://www.utwente.nl/">University of Twente</a>, The Netherlands)
	</li>
	<li>
		Technical setup &amp; tool execution:
		<a href="https://moves.rwth-aachen.de/people/quatmann/">Tim Quatmann</a>
		<script type="text/javascript">WriteMailLinkHtml('d.nehcaa-htwr.sc@nnamtauq.mit:otliame', '<img src="../../email.png" style="width: 13px; height: 11px; vertical-align: middle; margin-left: 4px;" />');</script>
		(<a href="https://www.rwth-aachen.de/">RWTH Aachen</a>, Germany)
	</li>
</p>

</body>
</html>